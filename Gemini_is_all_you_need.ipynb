{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3_XOAOas3FQZ",
        "uKwUNsRM3qWe",
        "Ts3pV9gW4OeQ",
        "TO5vklU-4nza",
        "jb3OlEAg5Ctj",
        "1J2TEKtR5ZDX",
        "Yh8jTjO4_H9i",
        "SIrBktMm_eb3"
      ],
      "authorship_tag": "ABX9TyN2KnlgEwj4OGqVyXdAUceQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvarDev/Gemini-is-all-you-need/blob/main/Gemini_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b9G14LFX3Tb"
      },
      "outputs": [],
      "source": [
        "# Based on https://github.com/GoogleCloudPlatform/generative-ai/tree/main\n",
        "\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini é tudo o que você precisa\n",
        "\n",
        "[Add Shorcuts to open by their own]"
      ],
      "metadata": {
        "id": "sJh1zqTAzmxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Este notebook mostra diferente cénarios de como o Gemini pode ser utilizado no dia a dia dos desenvolvedores.\n",
        "\n",
        "A temática é de futebol.\n",
        "\n",
        "Este notebook usa exemplos de outros notebooks no [GitHub do Google Cloud](https://github.com/GoogleCloudPlatform/generative-ai/tree/main).\n",
        "\n"
      ],
      "metadata": {
        "id": "5ML_HdO50no2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custos\n",
        "\n",
        "Este tutorial usa componentes faturáveis ​​do Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* BigQuery\n",
        "\n",
        "Saiba mais sobre [preços da Vertex AI](https://cloud.google.com/vertex-ai/pricing) e [BigQuery](https://cloud.google.com/bigquery/pricing).\n",
        "\n",
        "Use a [calculadora de preços](https://cloud.google.com/products/calculator/)\n",
        "para gerar uma estimativa de custo com base no uso projetado."
      ],
      "metadata": {
        "id": "6bQ-8Dj91_qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Começamos"
      ],
      "metadata": {
        "id": "uV9CfC_q2-5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar o SDK da Vertex AI e BigQuery para Python\n"
      ],
      "metadata": {
        "id": "3_XOAOas3FQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install --upgrade --user google-cloud-aiplatform google-cloud-bigquery"
      ],
      "metadata": {
        "id": "50t_q1dF3f13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinicie o Runtime\n",
        "Para usar os pacotes recém-instalados neste runtime do Jupyter, você deve reiniciar o runtime. Você pode fazer isso executando a seguinte célula, que irá reiniciar o kernel atual.\n"
      ],
      "metadata": {
        "id": "uKwUNsRM3qWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "-xc97W5B4ECI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "Se você estiver executando este notebook no Google Colab, execute a célula a seguir para autenticar seu ambiente. Esta etapa não é necessária se você estiver usando o [Vertex AI Workbench](https://cloud.google.com/vertex-ai-notebooks?hl=en)."
      ],
      "metadata": {
        "id": "Ts3pV9gW4OeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "tsdzin-R4lA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defina as informações do projeto do Google Cloud e inicialize o Vertex AI\n",
        "Inicialize o SDK da Vertex AI para Python para seu projeto:"
      ],
      "metadata": {
        "id": "TO5vklU-4nza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define project information\n",
        "PROJECT_ID = \"sre-demos\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "BQ_PROJECT_ID = \"sre-demos\"  # @param {type:\"string\"}\n",
        "BQ_LINKED_DATASET = \"fifa23\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI and BigQuery\n",
        "from google.cloud import bigquery\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "client = bigquery.Client(project=BQ_PROJECT_ID)"
      ],
      "metadata": {
        "id": "aKN64KA24yho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "jb3OlEAg5Ctj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    Image,\n",
        "    Part,\n",
        ")"
      ],
      "metadata": {
        "id": "d3ZbcMDq5KSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clube de Regatas Ganondorf - usando texto com Gemini"
      ],
      "metadata": {
        "id": "1J2TEKtR5ZDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quem torce pelo Clube de Regatas Ganondorf?**\n",
        "\n",
        "*Clube de Regatas Ganondorf é um FIFA Ultimate Team criado por Alvaro e Luciano. Alvaro é o técnico do time e Luciano atua como presidente do time, Luciano decide os novos contratos dos jogadores e quem sai do time. O capitão da equipe é Marc-André ter Stegen e os jogadores mais populares são Evanilson, Thierry Henry, Luka Modrić e Ronaldo. A equipe CR Ganondorf venceu a Liga dos Campeões da UEFA em 2020, 2022 e 2023.*\n",
        "\n",
        "\n",
        "Essa informação não é 'pública' pois é informação gerada em casa, se perguntamos para o Gemini sobre o **CR Ganondorf** provavelmente a resposta vai ser muito diferente."
      ],
      "metadata": {
        "id": "KpuBTRbL8C_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Para esta primeira parte vamos o usar o Gemini 1.0 Pro"
      ],
      "metadata": {
        "id": "mZytBFHY8Pc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")"
      ],
      "metadata": {
        "id": "ljUrID8m7soz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos o promt"
      ],
      "metadata": {
        "id": "eSedN-Rw8agO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"O que é o Clube de Regatas Ganondorf?\"\n",
        "\n",
        "contents = [\n",
        "    prompt,\n",
        "]\n",
        "\n",
        "response = model.generate_content(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "7DJLV4bH8dfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A resposta (no momento que criei a demo):\n",
        "\n",
        "\"*O Clube de Regatas Ganondorf **não existe**. Ganondorf é um personagem fictício da série de videogames The Legend of Zelda.*\"\n",
        "\n",
        "Para indicar ao Gemini a respota desejada, temos que forneces um contexto no promt, vamos usar o conceito que vimos ao inicio.\n",
        "\n",
        "Adicionalmente podemos indicar ao Gemini responder num formato especifico, vamos testar com JSON:"
      ],
      "metadata": {
        "id": "ffqRoOgY_PCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vars names are not pre-defined, you can use your own vars\n",
        "\n",
        "context = \"\"\"\n",
        "            Clube de Regatas Ganondorf é um FIFA Ultimate Team criado por Alvaro e Luciano.\n",
        "            Alvaro é o técnico do time e Luciano atua como presidente do time,\n",
        "            Luciano decide os novos contratos dos jogadores e quem sai do time.\n",
        "            O capitão da equipe é Marc-André ter Stegen e os jogadores mais populares são\n",
        "            Evanilson, Thierry Henry, Luka Modrić e Ronaldo.\n",
        "            A equipe CR Ganondorf venceu a Liga dos Campeões da UEFA em 2020, 2022 e 2023.\n",
        "          \"\"\"\n",
        "\n",
        "questions = \"\"\"\n",
        "              Responde as seguintes perguntas:\n",
        "              O que é o Clube de Regatas Ganondorf?\n",
        "              Quem é o capitão do time?\n",
        "              Quem é o presidente do clube?\n",
        "            \"\"\"\n",
        "\n",
        "format =  \"\"\"\n",
        "            Em formato JSON:\n",
        "            {\n",
        "              \"question\": \"pergunta\",\n",
        "              \"answer\": \"resposta\"\n",
        "            }\n",
        "          \"\"\"\n",
        "\n",
        "contents = [\n",
        "    context,\n",
        "    questions,\n",
        "    format,\n",
        "]\n",
        "\n",
        "response = model.generate_content(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "K0YsdtcEAUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying Fifa 2023 Dataset no BigQuery"
      ],
      "metadata": {
        "id": "RaaUiubansIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste exemplo vamos usar o dataset [**Fifa 2023 EDA**](https://www.kaggle.com/datasets/anshulchauhan21/fifa-2023-dataset/data) do [Kaggle](https://www.kaggle.com).\n",
        "\n",
        "> O dataset FIFA 2023 é uma coleção abrangente de dados relacionados à popular franquia de videogame de futebol. Inclui informações sobre vários aspectos do jogo, como atributos do jogador, detalhes da equipe, classificações e muito mais.\n",
        "\n",
        "Vamos usar o [BigQuery](https://cloud.google.com/bigquery?hl=en) como engine de banco de dados, porém este exemplo pode ser aplicado a diferentes engines de bancos de dados.\n",
        "\n",
        "Criar o dataset no BigQuery não faz parte deste notebook, segue o link para o tutorial [Criar FIFA 23 Dataset](https://drive.google.com/file/d/19ARkbocNItJqE65CVh3tV2GskV1dCt7I/view).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W3q-T-lhoMeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando o prompt"
      ],
      "metadata": {
        "id": "IaDoM7m3qjYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para construir este prompt precisamos das seguintes partes:\n",
        "\n",
        "*   **instructions**: As indicações inicias que o Gemini deve seguir.\n",
        "*   **schema_prompt**: Indicação que apresenta o schema da tabela.\n",
        "*   **few_examples**: Alguns exemplos de como fazer as queries.\n",
        "*   **question**: Pergunta enviada pelo usuario.\n",
        "*   **answer**: Resposta do Gemini."
      ],
      "metadata": {
        "id": "RGhpwCamqrYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instructions"
      ],
      "metadata": {
        "id": "SJziCOF8tqpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"\"\"\n",
        "  Esta é uma tarefa que converte texto em instrução GoogleSQL.\n",
        "  Primeiro forneceremos o esquema do conjunto de dados e, em seguida, faremos uma pergunta em texto.\n",
        "  Você será solicitado a gerar uma instrução SQL válida para o BigQuery.\n",
        "  Remova quaisquer delimitadores em torno da resposta, como \"```\"\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "NBKPUm8bt2L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Schema Definition"
      ],
      "metadata": {
        "id": "jEwU8dl5uA8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![FIFA 23 Dataset schema definition](https://storage.googleapis.com/sre-demos-files/FIFA%2023%20dataset%20schema%20definition.png)\n"
      ],
      "metadata": {
        "id": "wk6lnr41MMqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BQ_TABLES=['players']\n",
        "QUERY = f\"\"\"\\\n",
        "SELECT\n",
        "    '[Schema (values)]: ' || '| log_summary | ' || STRING_AGG(table_values, ' | ') || ';' AS tables_definition,\n",
        "    '[Column names (type)]: ' || STRING_AGG(column_names_types) || ';' AS columns_definition\n",
        "FROM (\n",
        "    SELECT\n",
        "      table_name,\n",
        "      table_name || ' : ' || STRING_AGG(column_name, ' , ') as table_values,\n",
        "      STRING_AGG(table_name || ' : ' || column_name || ' (' || data_type || ')', ' | ') as column_names_types\n",
        "    FROM {BQ_PROJECT_ID}.{BQ_LINKED_DATASET}.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS\n",
        "    WHERE table_name IN {'(' + \",\".join(map(lambda x: f\"'{x}'\", BQ_TABLES)) + ')'}\n",
        "    GROUP BY table_name\n",
        "    ORDER BY table_name\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Create query job\n",
        "query_job = client.query(QUERY)\n",
        "# Get first row\n",
        "schema = next(query_job.result())\n",
        "\n",
        "# Build schema definition\n",
        "schema_prompt = f\"\"\"\n",
        "  Definição de esquema de tabelas do BigQuery:\n",
        "  {schema.tables_definition}\n",
        "\n",
        "  {schema.columns_definition}\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "print(schema_prompt)"
      ],
      "metadata": {
        "id": "FwWBkLQNuFVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few Examples\n",
        "\n",
        "Criamos alguns exemplos de como fazer a query"
      ],
      "metadata": {
        "id": "czgEAPdFxAkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_examples = \"\"\"\n",
        "  Aqui estão alguns exemplos:\n",
        "\n",
        "  Pergunta: Lista o ID, nome, numero de camisa, idade, nacionalidade e potencial  dos jogadores do Real Madrid CF?\n",
        "\n",
        "  Resposta: select ID, Name as nome , Kit_Number as numero_de_camisa, age as idade, Nationality as nacionalidade, Potential as potencial  from `players` where Club like \"%Real Madrid%\"\n",
        "\n",
        "  Pergunta: Quem é o jogador com maior potencial do Boca Juniors FC?\n",
        "\n",
        "  Resposta: select ID, Name as nome , Kit_Number as numero_de_camisa, age as idade, Nationality as nacionalidade, Potential as potencial from `players` where Club like \"%Boca Juniors%\" order by Potential DESC Limit 1\n",
        "\n",
        "  Pergunta: Qual era a meia de idades dos jogadores do Boca Juniors FC?\n",
        "\n",
        "  Resposta: select AVG(age) as idade from `players` where Club like \"%Boca Juniors%\"\n",
        "\"\"\"\n",
        "\n",
        "print(few_examples)"
      ],
      "metadata": {
        "id": "EdgyXaR8xGWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gerar consultas SQL"
      ],
      "metadata": {
        "id": "tfw_hjWN-_Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definir função auxiliar para gerar SQL\n"
      ],
      "metadata": {
        "id": "Yh8jTjO4_H9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguinte função auxiliar `generate_sql()` é usada para recuperar uma consulta SQL do modelo Vertex AI LLM usando o modelo de prompt que construímos até agora.\n",
        "\n",
        "Observe como `generate_sql()` usa a função `sanitize_output()` para reduzir a resposta à própria consulta SQL antes de retornar os resultados. Mesmo que o prompt do modelo inclua instruções para ajustar a saída do modelo, ainda pode haver aspas ou crases de bloco de código que precisam ser removidos para evitar um erro de sintaxe SQL subsequente."
      ],
      "metadata": {
        "id": "advwqRit_MZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# Strip text to include only the SQL code block with\n",
        "def sanitize_output(text: str) -> str:\n",
        "    # Strip whitespace and any potential backticks enclosing the code block\n",
        "    text = text.strip()\n",
        "    regex = re.compile(r\"^\\s*```(\\w+)?|```\\s*$\")\n",
        "    text = regex.sub(\"\", text).strip()\n",
        "\n",
        "    # Find and remove any trailing quote without corresponding opening quote\n",
        "    if re.search(r'^[^\"]*\"$', text):\n",
        "        text = text[:-1]\n",
        "    # Find and remove any leading quote without corresponding closing quote\n",
        "    if re.search(r'^\"[^\"]*$', text):\n",
        "        text = text[1:]\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Call model using prompt and pre-defined parameters\n",
        "def generate_sql(\n",
        "    model,\n",
        "    contents,\n",
        "    temperature: float = 0.2,\n",
        "    max_output_tokens: int = 1024,\n",
        "    top_k: int = 40,\n",
        "    top_p: float = 0.8,\n",
        ") -> str:\n",
        "    print(\"Generating SQL...\")\n",
        "    print(\"Number of input tokens: \" + str(len(prompt)))\n",
        "\n",
        "    # response = model.predict(\n",
        "    #     prompt,\n",
        "    #     temperature=temperature,\n",
        "    #     max_output_tokens=max_output_tokens,\n",
        "    #     top_k=top_k,\n",
        "    #     top_p=top_p,\n",
        "    # )\n",
        "\n",
        "    response = model.generate_content(contents)\n",
        "\n",
        "    text = response.text\n",
        "    print(\"Number of output tokens: \" + str(len(text)))\n",
        "    print(\"Response:\")\n",
        "    print(text)\n",
        "\n",
        "    # Strip text to include only the SQL code block\n",
        "    text = sanitize_output(text)\n",
        "    print(\"Response stripped:\")\n",
        "    print(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "9EnwOkCl_ceX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definir função auxiliar para executar SQL"
      ],
      "metadata": {
        "id": "SIrBktMm_eb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguinte função auxiliar `execute_sql()` é usada para executar uma consulta SQL no dataset ativo do BigQuery e retornar resultados como um dataframe.\n",
        "\n",
        "Observe como `execute_sql()` garante a qualificação dos nomes das tabelas com o projeto e o dataset do BigQuery especificado acima, antes de executar a consulta SQL."
      ],
      "metadata": {
        "id": "_QcE93pQ_iK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit number of bytes processed as a guardrail for cost control\n",
        "BQ_MAX_BYTES_BILLED = pow(2, 30)  # 1GB\n",
        "\n",
        "\n",
        "def execute_sql(query: str):\n",
        "    print(\"Executing SQL...\")\n",
        "\n",
        "    # Qualify table names with your project and dataset ID\n",
        "    for table_name in BQ_TABLES:\n",
        "        query = query.replace(\n",
        "            table_name, f\"{BQ_PROJECT_ID}.{BQ_LINKED_DATASET}.{table_name}\"\n",
        "        )\n",
        "\n",
        "    print(\"Query:\")\n",
        "    print(query)\n",
        "\n",
        "    # Validate the query by performing a dry run without incurring a charge\n",
        "    job_config = bigquery.QueryJobConfig(use_query_cache=False, dry_run=True)\n",
        "    try:\n",
        "        response = client.query(query, job_config=job_config)\n",
        "    except Exception as e:\n",
        "        print(\"Error validating query:\")\n",
        "        print(e)\n",
        "        return e\n",
        "\n",
        "    print(\"Query will process {:.2f} KB.\".format(response.total_bytes_processed / 1024))\n",
        "\n",
        "    # Execute the query\n",
        "    job_config = bigquery.QueryJobConfig(\n",
        "        use_query_cache=False, maximum_bytes_billed=BQ_MAX_BYTES_BILLED\n",
        "    )\n",
        "    try:\n",
        "        response = client.query(query)\n",
        "        df = response.to_dataframe()\n",
        "    except Exception as e:\n",
        "        print(\"Error executing query:\")\n",
        "        print(e)\n",
        "        return e\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "w_yBZISr_q2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vamos là"
      ],
      "metadata": {
        "id": "XPWhMfXE_stD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos gerar o SQL para responder a este exemplo de pergunta:"
      ],
      "metadata": {
        "id": "Ry7CV1Hg_yRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"  Escreva a consulta GoogleSQL para a seguinte pergunta: {user_question}\"\n",
        "answer = \"  Resposta: \\\"Query here\\\"\"\n",
        "\n",
        "user_question = \"Qual é a meia do potencial do real madrid?\"\n",
        "\n",
        "question = question.format(\n",
        "    user_question=user_question\n",
        ")\n",
        "\n",
        "contents = [\n",
        "    instructions,\n",
        "    schema_prompt,\n",
        "    few_examples,\n",
        "    question,\n",
        "    answer,\n",
        "]\n",
        "\n",
        "query = generate_sql(\n",
        "    model,\n",
        "    contents\n",
        "    )\n",
        "\n",
        "print(query)"
      ],
      "metadata": {
        "id": "3Rc6F1FH_zas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos testar a consulta gerada com o dataset ativo no BigQuery."
      ],
      "metadata": {
        "id": "3un-nmeyCStH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the query\n",
        "query_result = execute_sql(query)\n",
        "display(query_result)"
      ],
      "metadata": {
        "id": "h9U_W1xNCTgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying Imagens"
      ],
      "metadata": {
        "id": "2xPpUVSoSTHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agora vamos usar Gemini 1.0 Pro Vision"
      ],
      "metadata": {
        "id": "i7ySfV9TSWub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")"
      ],
      "metadata": {
        "id": "drb43pslTDZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definir funções auxiliares para imagens"
      ],
      "metadata": {
        "id": "lIK9Zy1vTTUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import http.client\n",
        "import typing\n",
        "import urllib.request\n",
        "\n",
        "import IPython.display\n",
        "from PIL import Image as PIL_Image\n",
        "from PIL import ImageOps as PIL_ImageOps\n",
        "\n",
        "\n",
        "def display_images(\n",
        "    images: typing.Iterable[Image],\n",
        "    max_width: int = 600,\n",
        "    max_height: int = 350,\n",
        ") -> None:\n",
        "    for image in images:\n",
        "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
        "        if pil_image.mode != \"RGB\":\n",
        "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
        "            pil_image = pil_image.convert(\"RGB\")\n",
        "        image_width, image_height = pil_image.size\n",
        "        if max_width < image_width or max_height < image_height:\n",
        "            # Resize to display a smaller notebook image\n",
        "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
        "        IPython.display.display(pil_image)\n",
        "\n",
        "\n",
        "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
        "    with urllib.request.urlopen(image_url) as response:\n",
        "        response = typing.cast(http.client.HTTPResponse, response)\n",
        "        image_bytes = response.read()\n",
        "    return image_bytes\n",
        "\n",
        "\n",
        "def load_image_from_url(image_url: str) -> Image:\n",
        "    image_bytes = get_image_bytes_from_url(image_url)\n",
        "    return Image.from_bytes(image_bytes)\n",
        "\n",
        "\n",
        "def display_content_as_image(content: str | Image | Part) -> bool:\n",
        "    if not isinstance(content, Image):\n",
        "        return False\n",
        "    display_images([content])\n",
        "    return True\n",
        "\n",
        "\n",
        "def display_content_as_video(content: str | Image | Part) -> bool:\n",
        "    if not isinstance(content, Part):\n",
        "        return False\n",
        "    part = typing.cast(Part, content)\n",
        "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
        "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
        "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
        "    return True\n",
        "\n",
        "\n",
        "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
        "    \"\"\"\n",
        "    Given contents that would be sent to Gemini,\n",
        "    output the full multimodal prompt for ease of readability.\n",
        "    \"\"\"\n",
        "    for content in contents:\n",
        "        if display_content_as_image(content):\n",
        "            continue\n",
        "        if display_content_as_video(content):\n",
        "            continue\n",
        "        print(content)"
      ],
      "metadata": {
        "id": "LMBYtDhVTZHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vamos entender uma imagem"
      ],
      "metadata": {
        "id": "46JFOCH7Td19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O CR Ganondorf não existe no dataset do FIFA 23, porém podemos ver o potencial de cada jogador diretamente no videogame, vamos obter a media do potencial to time usando a imagem com essa informação:\n",
        "\n",
        "![CR Ganondorf](https://storage.googleapis.com/sre-demos-files/cr_ganondorf_team.jpeg)"
      ],
      "metadata": {
        "id": "VWFeSZT4Tqs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr_ganondorf_url = \"https://storage.googleapis.com/sre-demos-files/cr_ganondorf_team.jpeg\"\n",
        "cr_ganondorf_image = load_image_from_url(cr_ganondorf_url)\n",
        "\n",
        "image_instructions = \"\"\"\n",
        "  Instruções: Considere a seguinte imagem que contém jogadores de futebol:\n",
        "\"\"\"\n",
        "\n",
        "prompt1 = \"\"\"\n",
        "  Cada área dourada representa a informação de cada jogador\n",
        "  O potencial é o numero que esta na parte esquerda do rosto do jogador.\n",
        "  A posição é as letras embaixo do potencial.\n",
        "\n",
        "  Mostra numa tabela o numero de ordem, o potencial e a posição.\n",
        "\n",
        "  Qual é a meia do potencial do time?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "contents = [\n",
        "    image_instructions,\n",
        "    cr_ganondorf_image,\n",
        "    prompt1,\n",
        "]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "Markdown(responses.text)"
      ],
      "metadata": {
        "id": "lrkPilb3UMAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Videos prompt\n"
      ],
      "metadata": {
        "id": "gxHAls6eFkX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Responde as perguntas o mais preciso possivel usando o contexto do video.\n",
        "Se a resposta nao estiver no contexto, responde \"resposta nao disponivel no contexto\"\n",
        "\n",
        "Contexto:\n",
        "O video é um jogo de futebol entre os times\n",
        "\n",
        "O time UNI usa camisa branca, calção e meiões brancos.\n",
        "O time ALI usa Camisa azul marinho com detalhes em branco, calção e meiões azuis marinho.\n",
        "\n",
        "Descreve os goals feitos no video\n",
        "\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://sre-demos-files/lima.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Response--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ],
      "metadata": {
        "id": "ZWrH94DKFreP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}